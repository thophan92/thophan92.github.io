---
layout: post
title: "Unified Language Model Pre-training for Natural Language Understanding and Generation"
comments: true
description: "short description"
showtags: true
tags: nlp nlu nlg neurips
---

### Key ideas
- Train multi-layer transformer network with 4 different types of LMs: Left-to-Right LM (1/6), Right-to-Left LM (1/6), Bidirectional LM (1/3) and Seq2seq LM (1/3)
- All parameters in 4 types LM are shared together.

### Contribution
- introduce UNILM that can apply to both NLU and NLG tasks

### A deeper look
- `input`: sequence $$x = x_{1} ... x_{n}$$
- `output`: contextualized vector representation for each token
- A modification for self-attention: They provide a variable $$M$$ to control what context a token can attend to compute its representation. 
$$\mathbf{M}_{ij}=\left\{\begin{array}{l}{0,  allow}\\ {-\infty,  prevent}\end{array}\right.$$


#### Some measurements
- It shows a good result in abstractive text summarization
- Other task performance is not as good as other models such as LXNet, RoBERTa.


### [Original paper](http://papers.nips.cc/paper/9464-unified-language-model-pre-training-for-natural-language-understanding-and-generation.pdf) and [Source code](https://github.com/microsoft/unilm)
